[ { "title": "기존 git 소스를 새로운 git repo로 복사하기", "url": "/posts/git-remote-set-url/", "categories": "Git, Cases", "tags": "git", "date": "2022-11-29 10:00:00 +0900", "snippet": "기존 git 소스를 새로운 git repo로 복사하기상황기존에 쓰던 git 소스를 새로운 repo로 옮겨야 하는 상황이 생겼다. 그래서 git remote 명령어를 통해 옮겨보고자 한다.해결 방법아래와 같이 git 명령어를 통해 a-repo에 있던 모든 소스를 b-repo로 옮긴다. # a-repo를 clone한 dir로 이동cd a-repo# 원격 저장소 목록을 확인하는 명령어git remote-&gt; 예시 출력) origin# origin이라는 원격 저장소의 url을 가져오는 명령어git remote get-url origin-&gt; 예시 출력) https://git.com/a-repo.git# origin이라는 원격 저장소의 url을 새로운 url로 변경하는 명령어# git remote set-url [원격저장소명] [new-repo-url]git remote set-url orgin https://git.com/b-repo.git# 변경됐는지 확인하는 명령어git remote get-url origin-&gt; 예시 출력) https://git.com/b-repo.git" }, { "title": "Nginx Ingress Controller에서 Client IP 유지하게 설정하기", "url": "/posts/k8s-ic-use-forwarded-header/", "categories": "Kubernetes, Cases", "tags": "k8s, ingress", "date": "2022-11-28 10:00:00 +0900", "snippet": "Nginx Ingress Controller에서 Client IP 유지하게 설정하기상황http_x_forwarded_for 헤더 값으로 Client IP를 받아와 사용하는 로직이 있는 서비스가 있다.그런데 Client IP가 Ingress Controller를 거치면서 http_x_forwarded_for 헤더 값이 Ingress Controller Pod IP로 변경되어 해당 서비스로 들어와 에러가 발생했다.그래서 Client IP가 서비스 Pod까지 유지되게 설정을 해야 한다.해결 방법해당 헤더 값을 유지하려면 use-forwarded-header를 true로 변경해줘야 한다.기본 값은 use-forwarded-header: “false”다. 아래와 같이 ingress-controller pod가 사용하는 ConfigMap을 변경한다. kubectl edit cm -n ingress-nginx ingress-nginx-controller apiVersion: v1kind: ConfigMapmetadata: name: ingress-nginx-controller namespace: ingress-controllerdata: allow-snippet-annaotations: \"true\" # 새로 추가할 부분 use-forwarded-headers: \"true\"... ConfigMap을 수정하면 ingress-controller Pod에 바로 적용된다.위와 같이 적용해주면 Ingress Controller를 거쳐도 Client의 IP가 서비스 Pod까지 유지되어 http_x_forwarded_for 헤더 값에 남게 된다.참고 https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#use-forwarded-headers" }, { "title": "kubectl cp 명령어 ownership 에러 해결하기", "url": "/posts/k8s-cp-ownership-error/", "categories": "Kubernetes, Cases", "tags": "k8s", "date": "2022-11-27 10:00:00 +0900", "snippet": "kubectl cp 명령어 ownership 에러 해결하기상황kubectl cp 명령어로 bastion host에 있는 파일을 pod에 마운트 되어있는 file system 디렉토리로 복사할 때 다음과 같은 에러가 발생했다. 에러 내용 tar: 파일명: Cannot change ownership to uid xxxx, gid xxxx: Operation not permittedtar: Exiting with failure status due to previous errorscommand terminated with exit code 2 해결 방법위 에러는 bastion host에 있던 파일을 file system으로 옮기면서 file system의 소유자로 권한을 바꿔줘야 하는데 실패해서 발생하는 에러였다.kubectl cp 명령어를 사용할 때 파일의 소유자를 그대로 유지하는 것이 기본 값이다.그러므로 kubectl cp 명령어를 사용할 때 –no-preserve=true 파라미터를 넣어주어 파일의 소유자를 유지하지 않게 하면 된다. 기본 값은 –no-preserve=false " }, { "title": "폐쇄망에서 terraform 사용하기", "url": "/posts/terraform-private-network/", "categories": "Terraform, Cases", "tags": "terraform, aws", "date": "2022-11-26 11:00:00 +0900", "snippet": "폐쇄망에서 terraform 사용하기상황폐쇄망에서 Terraform으로 AWS에 인프라를 구성하려고 하니 아래와 같이 terraform 공식 registry에 접속할 수 없어 에러가 발생한다.그러므로 필요한 provider 패키지 바이너리 파일을 직접 다운 받아 로컬에 저장해놓고 terraform을 실행할 수 있게 해야 한다.AWS Provider 패키지 다운로드하기terraform에서 쓰는 provider 패키지 바이너리 파일은 아래 URL에서 다운받을 수 있다. https://releases.hashicorp.com/terraform-provider-aws/4.48.0/ aws 자리에 다른 provider를 넣고 4.48.0 자리에 다른 버전을 넣어서 원하는 파일을 다운로드 할 수 있다.terraform init 시, 로컬 경로에서 다운받게 설정하기1.위에서 다운받은 zip 파일을 terraform을 실행하는 서버로 옮긴다. 압축을 풀고 아래 경로로 복사한다. unzip terraform-provider-aws_4.48.0_linux_amd64.zip provider 패키지를 다운받을 경로 mkdir -p /usr/share/terraform/providers/registry.terraform.io/hashicorp/aws/4.48.0/linux_amd64/cp ./terraform-provider-aws_v4.48.0_x5 /usr/share/terraform/providers/registry.terraform.io/hashicorp/aws/4.48.0/linux_amd64/ plugin cache 경로 mkdir -p ~/.terraform.d/plugins/registry.terraform.io/hashicorp/aws/4.48.0/linux_amd64/cp ./terraform-provider-aws_v4.48.0_x5 ~/.terraform.d/plugins/registry.terraform.io/hashicorp/aws/4.48.0/linux_amd64/2.terraformrc 파일을 생성하여 공식저장소 대신 다운받을 경로를 지정한다. $ vi ~/.terraformrc# plugin cache 저장 경로plugin_cache_dir = \"$HOME/.terraform.d/plugin-cache\"disable_checkpoint = trueprovider_installation {\tfilesystem_mirror {\t # 공식저장소 대신 init시 패키지를 다운받을 경로\t\tpath = \"/usr/share/terraform/providers\"\t\tinclude = [\"registry.terraform.io/hashicorp/*\",.....]\t}\tdirect {\t\texclude = [\"registry.terraform.io/hashicorp/*\",.....]\t}}terraform init 테스트vi ~/terraform-test/aws-ec2/main.tfterraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"&gt;= 3.0.0\" } } required_version = \"&gt;= 1.2.0\"}provider \"aws\" { region = \"us-west-2\"}resource \"aws_instance\" \"app_server\" { ami = \"ami-830c94e3\" instance_type = \"t2.micro\" tags = { Name = \"ExampleAppServerInstance\" }} $ terraform init에러가 발생하지 않고 init에 성공했다.그럼 이제 AWS VPC부터 생성해보자.참고 https://developer.hashicorp.com/terraform/tutorials/aws-get-started https://developer.hashicorp.com/terraform/tutorials/aws-get-started/aws-build#troubleshooting https://releases.hashicorp.com/terraform-provider-aws/4.48.0/" }, { "title": "Nginx if문 사용하기", "url": "/posts/nginx-if/", "categories": "Nginx, Cases", "tags": "nginx", "date": "2022-11-25 11:00:00 +0900", "snippet": "Nginx if문 사용하기상황특정 키워드가 포함된 요청을 다른 url로 proxy pass 하거나 401, 403과 같은 에러를 발생하게 해달라는 요청이 있었다.nginx.conf에 if문을 사용하면 위와 같이 특정 상황에 대한 처리를 다르게 할 수 있다.nginx.conf에 if문 설정 방법 test라는 키워드가 포함된 uri가 요청 들어왔을 때 403에러를 발생시키는 설정...location / { if ($uri ~ \"(.*)(test)(.*)\"){ return 403;\t }... test라는 키워드가 포함된 uri가 요청 들어왔을 때 다른쪽으로 proxy pass 하기...location /{ set &amp;proxy_pass_url \"http://origin.com\";\tif ($uri ~ \"(.*)(test)(.*)\"){\t set $proxy_pass_url \"http://test.com\";\t}\tproxy_pass $proxy_pass_url;...위와 같이 설정하면 특정 상황에 따라 다른 처리를 할 수 있다." }, { "title": "Java Pod에 OOM 발생시 Heap Dump 생성하는 설정하기", "url": "/posts/k8s-oom-heap-dump/", "categories": "Kubernetes, Cases", "tags": "k8s, java", "date": "2022-11-24 11:00:00 +0900", "snippet": "Java Pod에 OOM 발생시 Heap Dump 생성하는 설정하기상황Java 기반의 Pod가 OOM이 발생하여 재생성 되었는데 이유가 무엇인지 확인할 수 없어 OOM killed 되었을 때 Heap Dump를 생성하는 옵션을 설정하고자 한다.설정 방법 Pod 및 ConfigMap 설정apiVersion: apps/v1kind: Deploymentmetadata: labels: app: test name: test-deploy namespace: test-ns...spec:... containers: - envFrom: - configMapRef: name: test-configmap\t volumeMounts: - mountPath: /fs name: test-fs\t\t volumes: - name: test-fs persistentVolumeClaim: claimName: test-pvc\t\t ...---apiVersion: v1kind: ConfigMapmetadata: name: test-configmap namespace: test-nsdata: JAVA_OPT: \"-Dspring.profiles.active=dev -Duser.timezone=Asia/Seoul\" HEAP_OPT: \"-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/fs/heap_dump/${HOSTNAME}.hprof\"\t\t # OOM 발생시 Heap Dump를 생성하겠다는 옵션 # Pod의 HOSTNAME은 Pod명이므로 Pod이름으로 해당 경로에 Heap dump 파일을 생성한다. entrypoint.sh 설정#!/bin/shjava ${HEAP_OPT} -jar ${JAVA_OPT} /deploy/app.jar위와 같이 entrypoint.sh에 java 실행 옵션으로 ConfigMap에 설정한 HEAP_OPT를 주면 Pod가 실행될 때 Heap dump를 생성하는 옵션이 적용된다.이제 OOM killed가 발생했을 때 Pod에 마운트한 File system에 dump가 저장될 것이다. " }, { "title": "AWS Opensearch Timezone 설정하기", "url": "/posts/aws-os-timezone/", "categories": "AWS, Opensearch", "tags": "aws, opensearch", "date": "2022-11-23 10:00:00 +0900", "snippet": "AWS Opensearch Timezone 설정하기상황AWS Opensearch를 사용하여 EKS 로그를 수집하고 Opensearch Dashboard를 통해 로그를 모니터링 한다.그러나 Opensearch Dashboard에서 로그를 확인하면 @timestamp가 Table 형식에서는 KST로 출력되고, JSON 형식에서는 UTC로 출력이 되는 문제를 발견했다.JSON 형식으로 데이터를 뽑아서 써야 하기 때문에 다음과 같은 방법으로 Timezone을 KST로 맞추었다.Opensearch Dashboard의 Timezone 적용 방식먼저, Opensearch Dashboard에서 JSON 형식으로 보는 로그 데이터와 Devtool 또는 Rest API호출을 통해 보는 로그 데이터는 Opensearch에 저장된 원본 데이터다.그래서 Opensearch에 데이터가 저장될 때 UTC로 저장되면 UTC로 출력되고, KST로 저장되면 KST로 출력된다.그러나 Opensearch Dashboard의 Table 형식으로 보는 로그 데이터는 Opensearch Dashboard의 Stack Management/Advanced Settings/Timezone for date formatting에서 설정한 Timezone 옵션값에 따라 원본 데이터가 바뀌어 출력된다.즉, Opensearch에 인덱싱이 될 때 UTC로 저장된 후 Stack Management/Advanced Settings/Timezone for date formatting에서 KST로 설정하면 Table 형식에서만 원본 데이터인 UTC에 +9를 한 KST로 보인다는 것이다.또한 Opensearch에 인덱싱이 될 때 KST로 저장된 후 Stack Management/Advanced Settings/Timezone for date formatting에서도 KST로 설정하면 Table 형식으로 로그를 찾으려면 KST 기준시간 +9를 한 시간대에서 로그를 검색해야 한다. #예시Opensearch Indexing -&gt; KST 09:00:00-&gt; Opensearch Dashboard에서 Timezone을 KST로 설정 -&gt; Opensearch에 저장된 데이터에 Timezone 세팅까지 이중으로 하는 셈이다.-&gt; 그래서 KST 18:00:00에서 찾아야 해당 로그를 확인할 수 있다.이런식으로 동작되기 때문에 Opensearch Timezone 설정을 하려면 몇 가지 선택을 해야 한다.Opensearch 로그의 timestamp를 KST로 출력하기1.원본 데이터는 UTC로 저장하고 Opensearch Dashboard에서 Timezone을 설정하여 Table 형식에서만 KST로 로그 보기 opensearch에 인덱싱 하는 lambda 코드 중 아래의 코드는 @timestamp를 UTC로 저장한다.source[‘@timestamp’] = new Date(1 * logEvent.timestamp).toISOString();위와 같이 Date의 toISOString은 UTC가 기본이라고 한다. 그러므로 위와 같이 UTC로 인덱싱한 후 Opensearch Dashboard에서만 Timezone을 KST로 변경해준다.방법은 Opensearch Dashboard에 들어가서 좌측 상단의 메뉴를 누르고 Stack Management/Advanced Settings/Timezone for date formatting 경로로 들어가 KST로 변경 후 적용한다.그럼 Opensearch Dashboard Discover에서 로그를 검색할 때 KST 기준으로 검색하면 Table 형식의 로그에서는 timestamp가 KST로 출력된다.다만 이런 경우에는 JSON 형식으로 확인하면 UTC로 출력된다.2.원본 데이터를 KST로 저장하고 Opensearch Dashboard는 UTC로 설정하기 opensearch에 인덱싱 하는 lambda 코드 중 아래의 코드는 @timestamp를 KST로 저장한다.source[‘@timestamp’] = moment(logEvent.timestamp).tz(‘Asia/Seoul’).format(‘YYYY-MM-DDTHH:mm:ss.SSS[Z]’); moment 라이브러리를 추가하여 쓰는 방법은 여기서 확인할 수 있다. -&gt; 위와 같이 moment 함수를 사용하여 opensearch 자체에 인덱싱을 KST로 하면 JSON이나 Devtool에서도 KST로 출력되는 로그를 볼 수 있다.그러나 이때 몇 가지 더 설정을 해주어야 한다. Stack Management/Advanced Settings/Timezone for date formatting 경로로 들어가 UTC로 변경 후 적용한다.-&gt; KST에 +9시간이 되는 것을 막기 위해 Opensearch Dashboard에서 Discover로 검색할 때 now가 아닌 절대 시간을 입력해야 해당 시간의 로그가 검색된다.-&gt; 아마 Opensearch Dashboard는 UTC로 돼있고 로그 값은 KST라서 현재 시간으로 찾으면 안 나오고 시간 검색에 절대값을 넣어야 검색이 가능한 것 같다. 매번 절대값을 넣어 검색하는 게 번거롭긴 하지만 어쩔 수 없는 것 같다.차라리 UTC로 데이터를 뽑아서 데이터를 사용할 때 +9시간을 해주는 게 더 나을 수도 있을 것 같다.^^참고 [https://aws.amazon.com/ko/blogs/big-data/set-advanced-settings-with-the-amazon-opensearch-service-dashboards-api/] (https://aws.amazon.com/ko/blogs/big-data/set-advanced-settings-with-the-amazon-opensearch-service-dashboards-api/)" }, { "title": "EKS 기존 노드로 AMI를 생성하여 새로운 노드그룹 생성하기", "url": "/posts/eks-workernode-ami/", "categories": "AWS, EKS", "tags": "AWS, eks", "date": "2022-11-22 10:00:00 +0900", "snippet": "EKS 기존 노드로 AMI를 생성하여 새로운 노드그룹 생성하기상황AWS EKS를 사용하던 중 기존 노드(인스턴스)에 설치된 솔루션 및 설정들을 그대로 새 노드에 적용해야 하는 일이 생겼다.애초에 노드 그룹을 생성할 때 템플릿에 필요한 설정들을 다 넣어놨으면 좋았겠지만, 중간에 추가된 것들이라 기존 노드로 이미지(ami)를 만들어서 새 노드그룹을 생성해야 한다.EKS 노드(인스턴스) 이미지(AMI) 생성하기1.아래의 경로로 인스턴스 상세 페이지로 접속한다. EKS 콘솔 -&gt; 클러스터 -&gt; 컴퓨팅 -&gt; 복사하고자 하는 노드 선택 -&gt; 인스턴스 ID 클릭 -&gt; 인스턴스 상세 페이지 진입2.인스턴스로 이미지(AMI)를 생성한다. 우측 상단의 작업 버튼 클릭 -&gt; 이미지 및 템플릿 클릭 -&gt; 이미지 생성 클릭 -&gt; 이미지 이름, 설명, 태그 등 추가하고 생성하기 클릭3.생성된 이미지(AMI)를 확인한다. EC2 콘솔 -&gt; 이미지 -&gt; AMI 접속 -&gt; 위에서 설정한 AMI 이름으로 검색하여 생성된 이미지 확인기존 노드의 AMI로 새 시작템플릿 생성하기1.다음과 같은 경로에서 시작템플릿을 생성할 수 있다. EC2 콘솔 -&gt; 인스턴스 -&gt; 시작템플릿 -&gt; 시작템플릿 생성2.시작템플릿 이름 및 설명 등을 작성하고 ‘애플리케이션 및 OS 이미지(Amazon Machine Image)’에서 위에서 생성한 AMI를 선택한다. 내 AMI 클릭 -&gt; 위에서 생성한 AMI 이름 검색 -&gt; 선택 -&gt; 나머지 사항 설정 -&gt; 시작템플릿 생성 버튼 클릭이렇게 EKS 노드그룹을 생성할 때 사용할 시작템플릿을 만들었으니 이제 이 시작템플릿을 가지고 새 노드그룹만 만들면 된다.새 노드그룹 생성하기1.다음과 같은 경로에서 노드그룹을 생성할 수 있다. EKS 콘솔 -&gt; 클러스터 -&gt; 컴퓨팅 -&gt; 노드그룹 -&gt; 우측 상단 노드그룹 추가 클릭2.시작템플릿 사용을 활성화하여 위에서 만든 시작템플릿을 선택한다. 노드그룹 이름, 역할을 설정한 뒤 시작템플릿을 선택한다. 그 후 여러 세부사항들을 설정하고 노드그룹을 생성한다.3.노드 생성 후 노드에 접속하여 원하는 설정들이 포함됐는지 확인한다. $ ssh -i EC2-key.pem workernode-ip주의사항새 노드를 생성한 후 해당 노드에 Pod를 배포하면 다음과 같은 에러가 발생한다. Error from server: no preferred addresses found; known addresses: []해당 시작템플릿은 기존 노드의 모든 것들을 가져왔기 때문에 kubelet 설정도 기존 노드의 IP로 되어있어 발생하는 에러다.그러므로 새 노드에 접속하여 아래 파일을 변경해줘야 한다. $ ssh -i EC2-key.pem workernode-ip$ vi /etc/systemd/system/kubelet.service.d/10-kubelet-args.confEnviroment=’KUBELET_ARGS=–node-ip=”이 부분에 적혀있는 IP를 새 노드 IP로 변경해줘야 한다”변경 후 systemctl restart kubelet을 해준다.그 후 Pod를 재배포 해보면 에러가 발생하지 않고 제대로 배포된 것을 확인할 수 있다." }, { "title": "Java 기반 Pod에 Heap Memory 설정하기", "url": "/posts/k8s-java-heap-setting/", "categories": "Kubernetes, Cases", "tags": "k8s, java", "date": "2022-11-21 10:00:00 +0900", "snippet": "Java 기반 Pod에 Heap Memory 설정하기상황성능테스트를 하던 도중 원하는 목표치에 도달하기도 전에 OOM(Out of Memory) 에러가 발생하여 Java 기반의 Pod의 Heap 사이즈를 변경해달라는 요청을 받아 변경하려 한다.해결 방법아래와 같이 java 실행 명령어에 HEAP_OPT 값을 주어 app.jar를 실행시켜야 한다. entrypoint.sh java ${HEAP_OPT} -jar ${JAVA_OPT} app.jar해당 HEAP_OPT를 ConfigMap에 설정하여 entrypoint.sh 가 실행될 때 같이 실행될 수 있도록 한다.apiVersion: v1kind: ConfigMapmetadata: name: test-configmap namespace: test-nsdata: TZ: Asia/Seoul JAVA_OPT: \"-Dspring.profiles.active=dev -Duser.timezone=Asia/Seoul\" HEAP_OPT: \"-XX:MinRAMPercentage=50 -XX:MaxRAMPercentage=80 -XshowSettings:vm\" 최소, 최대 Heap memory 값을 절대값으로 넣어주면 Pod의 Resource Limits 값이 달라질 때마다 그에 맞춰 변경을 해줘야하는 불편함이 있다. 그래서 MinRAMPercentage, MaxRAMPercentage 값을 통해 비율을 조정해주면 된다. 그리고 우리가 변경한 최소, 최대 Heap memory 사이즈가 제대로 적용됐는지 확인하기 위해 -XshowSettings:vm 값을 넣어준다. 그러면 Pod가 부팅될 때 로그에 세팅된 Heap 사이즈가 찍힌다. ex) Pod의 Resource Limits가 Memory 4G인 경우 Pod 실행을 위해 사용되는 memory 빼고 남는 memory 중 80인 약 3G 정도가 Heap Max 사이즈로 적용되었다." }, { "title": "K8s Pod에 Time Zone 설정하기", "url": "/posts/k8s-pod-tz-setting/", "categories": "Kubernetes, Cases", "tags": "k8s", "date": "2022-11-20 11:30:00 +0900", "snippet": "K8s Pod에 Time Zone 설정하기상황생성된 Pod의 로그를 kubectl logs 명령어로 확인하면 로그가 찍히는 시간이 현재 시간과 달라 에러를 디버깅할 때 어려움이 생겼다.그래서 Pod에 접속하여 /etc/localtime 을 확인해보니 UTC로 되어있었다.해결방안Pod에 Time Zone을 설정해서 한국 표준 시간으로 맞춰야 한다. Time Zone을 설정하는 방법은 여러 개가 있다.1. TZ 환경변수 설정하기ConfigMap을 이용하여 환경변수에 KST를 설정해준다.apiVersion: apps/v1kind: Deploymentmetadata: labels: app: test name: test-deploy namespace: test-nsspec: replicas: 1 selector: matchLabels: app: test strategy: type: RollingUpdate template: metadata: labels: app: test spec: containers: - image: 123123.dkr.ecr.ap-northeast-2.amazonaws.com/test-ecr:0.1.2 imagePullPolicy: Always name: test-container ports: - containerPort: 8080 name: p-8080 envFrom: - configMapRef: name: test-configmap---apiVersion: v1kind: ConfigMapmetadata: name: test-configmap namespace: test-nsdata: TZ: Asia/Seoul---위와 같이 TZ을 Asia/Seoul로 설정했으나 적용이 되지 않는다. 그래서 다음 방법으로 해결했다.2.노드의 zoneinfo를 마운트하여 localtime을 설정하기apiVersion: apps/v1kind: Deploymentmetadata: labels: app: test name: test-deploy namespace: test-nsspec: replicas: 1 selector: matchLabels: app: test strategy: type: RollingUpdate template: metadata: labels: app: test spec: containers: - image: 123123.dkr.ecr.ap-northeast-2.amazonaws.com/test-ecr:0.1.2 imagePullPolicy: Always name: test-container ports: - containerPort: 8080 name: p-8080 envFrom: - configMapRef: name: test-configmap volumeMounts: - name: tz-config mountPath: /etc/localtime volumes: - name: tz-config hostPath: path: /usr/share/zoneinfo/Asia/Seoul---apiVersion: v1kind: ConfigMapmetadata: name: test-configmap namespace: test-nsdata: TZ: Asia/Seoul위와 같은 방법으로 설정하니 Time Zone이 KST로 잘 적용되었다. 어떤 경우에는 base image에 /usr/share/zoneinfo가 없어서 해당 directory를 같이 마운트해줘야 KST가 적용되는 경우도 있다.apiVersion: apps/v1kind: Deploymentmetadata: labels: app: test name: test-deploy namespace: test-nsspec: replicas: 1 selector: matchLabels: app: test strategy: type: RollingUpdate template: metadata: labels: app: test spec: containers: - image: 123123.dkr.ecr.ap-northeast-2.amazonaws.com/test-ecr:0.1.2 imagePullPolicy: Always name: test-container ports: - containerPort: 8080 name: p-8080 envFrom: - configMapRef: name: test-configmap volumeMounts: - name: tz-data mountPath: /usr/share/zoneinfo - name: tz-config mountPath: /etc/localtime\t\t volumes: - name: tz-data hostPath: path: /usr/share/zoneinfo - name: tz-config hostPath: path: /usr/share/zoneinfo/Asia/Seoul\t\t ---&lt;br/&gt;그러나 혹시 spring 어플리케이션에서 위와 같은 방법으로도 적용이 안 된다면 JAVA 실행 옵션에 아래와 같이 설정해보자.**3. JVM 실행 옵션 주기**```yamlapiVersion: apps/v1kind: Deploymentmetadata: labels: app: test name: test-deploy namespace: test-nsspec: replicas: 1 selector: matchLabels: app: test strategy: type: RollingUpdate template: metadata: labels: app: test spec: containers: - image: 123123.dkr.ecr.ap-northeast-2.amazonaws.com/test-ecr:0.1.1 imagePullPolicy: Always name: test-container ports: - containerPort: 8080 name: p-8080 envFrom: - configMapRef: name: test-configmap volumeMounts: - name: tz-config mountPath: /etc/localtime volumes: - name: tz-config hostPath: path: /usr/share/zoneinfo/Asia/Seoul---apiVersion: v1kind: ConfigMapmetadata: name: test-configmap namespace: test-nsdata: TZ: Asia/Seoul JAVA_OPT: \"-Dspring.profiles.active=dev -Duser.timezone=Asia/Seoul\"위 ConfigMap에서 설정한 -Duser.timezone=Asia/Seoul는 Dockerfile의 entrypoint에서 java -jar ${JAVA_OPT}로 실행된다.세 가지 방법을 통해 로그가 KST로 나오는 것을 확인했다." }, { "title": "K8s PodAntiAffinity로 Pod의 중복 배포를 최소화 하기", "url": "/posts/k8s-pod-anti-affinity/", "categories": "Kubernetes, Cases", "tags": "k8s", "date": "2022-11-19 12:40:00 +0900", "snippet": "K8s PodAntiAffinity로 Pod의 중복 배포를 최소화 하기상황생성된 Worker Node가 3개, Pod의 최소 replica 수도 3개인데 어떤 Deployment는 하나의 노드에 3개의 Pod가 전부 배포되어 있고, 어떤 Deployment는 1,2번에만 pod가 배포되어 있는 걸 발견했다.이런 경우 어느 노드 또는 가용영역에 장애가 발생하게 되면 순간적으로 서비스 중단이 될 수 있기에 각 노드에 Pod가 골고루 배포되도록 설정할 필요가 있다.해결방안PodAntiAffinity를 사용하여 이미 같은 Deployment의 Pod가 배포된 노드에는 중복 배포가 되지 않도록 설정한다.podAntiAffinity 규칙은 스케줄러로 하여금 app=test 레이블을 가진 복수 개의 레플리카를 단일 노드에 배치하지 않게 한다. 이렇게 하여 Pod를 각 노드에 분산하여 생성한다.affinity.podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution.weight 값에 따라 어느 Node에 배포할지 가중치를 줄 수 있다.예를 들어 preferredDuringSchedulingIgnoredDuringExecution 규칙을 만족하는 노드가 2개 있고, 하나에는 app: test 레이블이 있고 다른 하나에는 type: nginx 레이블이 있으면, 스케줄러는 각 노드의 weight를 확인한 뒤 weight가 더 작은 쪽에 Pod를 배포한다.즉, app: test 레이블을 가진 pod-A가 배포된 node-A와 type: nginx 레이블을 가진 pod-B가 배포된 node-B가 있을 때 아래의 test-deploy는 가중치가 더 작은 app: test 레이블을 가진 pod-A가 배포된 node-A에 배포된다.또한 podAntiAffinity에서는 노드에 일관된 레이블을 지정해야 한다.즉, 클러스터의 모든 노드는 topologyKey 와 매칭되는 적절한 레이블을 가지고 있어야 한다. 일부 또는 모든 노드에 지정된 topologyKey 레이블이 없는 경우에는 의도하지 않은 동작이 발생할 수 있다. 예시apiVersion: apps/v1kind: Deploymentmetadata: labels: app: test name: test-deploy namespace: test-nsspec: replicas: 1 selector: matchLabels: app: test strategy: type: RollingUpdate template: metadata: labels: app: test spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 10 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - test\t\t topologyKey: \"kubernetes.io/hostname\" - weight: 50 podAffinityTerm: labelSelector: matchExpressions: - key: type operator: In values: - nginx\t\t\t\t topologyKey: \"kubernetes.io/hostname\" containers: - image: 123123123.dkr.ecr.ap-northeast-2.amazonaws.com/test:0.1.1 imagePullPolicy: Always name: test-container ports: - containerPort: 8080 name: p-8080 envFrom: - configMapRef: name: test-configmap resources: requests: memory: \"1G\" cpu: \"1\" limits: memory: \"1G\" cpu: \"1\" livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 30 periodSeconds: 60 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 30 periodSeconds: 60 주의: PodAffinity와 PodAntiAffinity에는 상당한 양의 프로세싱이 필요하기에 대규모 클러스터에서는 스케줄링 속도가 크게 느려질 수 있다. 수백 개의 노드를 넘어가는 클러스터에서 이를 사용하는 것은 추천하지 않는다.참고 https://kubernetes.io/ko/docs/concepts/scheduling-eviction/assign-pod-node/#more-practical-use-cases" }, { "title": "EKS Autoscaling Group 세부 설정하기", "url": "/posts/eks-autoscaling-setting/", "categories": "AWS, EKS", "tags": "aws, eks", "date": "2022-11-18 10:40:00 +0900", "snippet": "EKS Autoscaling Group 세부 설정하기상황EKS 노드그룹을 생성하면 해당 노드그룹의 Autoscaling을 담당하는 Autoscaling Group이 함께 생성되며 수시로 Workernode를 체크해서 Node의 수를 조절한다. (노드그룹에 지정한 desired, min, max를 가지고)그러나 어떠한 이유로 이미 생성된 노드가 변경되면 안 되는 상황이 있다. (ex. 보안 심의, 컷오버 등등)그런 상황에서도 테스트는 이뤄져야 하고 테스트 도중 노드가 새로 생기고 사라지기도 하는데 그때마다 이미 결재를 다 받아놓은 노드가 삭제되면 아주 난감한 상황이 발생한다.그래서 이미 결재 받은 노드 말고, 테스트 때문에 생긴 노드만 삭제되게 해야 했다.해결방안1.Autoscaling Group의 종료 정책을 변경한다.기본적으로 Autoscaling Group의 종료 정책은 제일 먼저 생긴 즉, 제일 오래된 인스턴스부터 종료하게 되어있다. 그래서 이것을 최신 인스턴스로 변경하면 제일 나중에 생긴 인스턴스부터 종료한다. EKS 콘솔 -&gt; Cluster 선택 -&gt; Node Group 선택 -&gt; Autoscaling Group 선택 -&gt; 세부정보 -&gt; 제일 하단의 고급 구성 편집 클릭 -&gt; 종료 정책에서 ‘가장 오래된 인스턴스’ 또는 ‘가장 오래된 시작템플릿’을 삭제 -&gt; ‘최신 인스턴스’ 추가 후 저장2.Autoscaling Group의 인스턴스 축소 보호를 설정한다. 인스턴스 축소 보호란 Auto Scaling 그룹에 대한 인스턴스 축소 보호 설정을 변경하여 축소 시 해당 Amazon EC2 Auto Scaling이 새 인스턴스를 종료할 수 있는지 여부를 제어할 수 있다.축소 보호가 활성화된 경우 새로 시작된 인스턴스는 기본적으로 축소 보호되지만 이미 생성된 인스턴스들은 따로 설정을 해야한다. 기생성된 인스턴스에 축소 보호 활성화 하기 EKS 콘솔 -&gt; Cluster 선택 -&gt; Node Group 선택 -&gt; Autoscaling Group 선택 -&gt; 인스턴스 관리 선택 -&gt; 축소 보호를 설정하려는 인스턴스 선택 -&gt; 우측 상단의 작업 버튼 클릭 -&gt; ‘축소 보호 설정’ 클릭*만약 이미 축소 보호가 설정된 인스턴스의 경우에는 ‘축소 보호 설정’ 버튼이 막혀있고, ‘축소 보호 제거’ 버튼만 활성화 되어있다. 참고 https://docs.aws.amazon.com/ko_kr/autoscaling/ec2/userguide/ec2-auto-scaling-instance-protection.html" }, { "title": "K8s Pod 이미지 Pull 에러 해결방안", "url": "/posts/k8s-image-pull-error/", "categories": "Kubernetes, EKS", "tags": "k8s, eks, docker", "date": "2022-11-17 10:40:00 +0900", "snippet": "K8s Pod 이미지 Pull 에러 해결방안상황Jenkins 파이프라인을 지우고 다시 생성하면서 버전이 같은 이미지가 생성되었다.ECR에서는 같은 버전의 이미지가 push 되면 기존에 있던 이미지를 untagged로 바꾸고 새로 push된 이미지를 해당 버전으로 표기하여 저장한다.그러나 EKS에서 해당 버전으로 배포를 다시 했더니 새로 빌드한 이미지가 아닌 예전에 생성된 이미지로 Pod가 배포되었다.ex) 22.11.01에 빌드한 ecr-test:0.1.2은 spring v2.2.2 -&gt; 22.11.05에 빌드한 ecr-test:0.1.2는 spring v2.6.0 -&gt; 그러나 22.11.05에 배포한 ecr-test:0.1.2 pod는 spring v2.2.2로 실행되고 있는 문제를 발견했다.원인EKS Worker Node에 기존 ecr-test:0.1.2 버전의 이미지가 남아있었으며, Pod의 imagePullPolicy가 IfNotPresent로 되어 있었다.그래서 Pod는 ecr-test:0.1.2 버전의 이미지가 worker node의 docker에 있으므로 새로 ECR에서 pull 해오지 않고 예전에 pull 해왔던 기존 이미지를 사용하여 Pod를 배포했던 것이다.해결방안1.먼저 Pod(또는 Deployment)의 yaml에 imagePullPolicy를 Always로 바꿔준다.apiVersion: v1kind: Podmetadata: name: test-podspec: containers: - name: spring image: ecr-test:0.1.2 imagePullPolicy: Always그러나 imagePullPolicy를 Always로 해준다고 해도 docker 이미지의 Digest 값이 같으면 다운로드를 시도는 하나 실제로 다운로드 하지 않는 현상이 있다고 한다.그러므로 worker node에 pull 해온 이미지들을 주기적으로 지워줌으로써 이전 이미지를 계속 쓰는 일이 없도록 해야 할 것이다.2.docker prune을 실행시키는 cron shell script를 사용하여 docker image를 주기적으로 지워준다.EKS의 경우 Worker Node가 생성될 때 Node Group의 시작템플릿에 cron shell을 생성하는 명령어를 추가해놓으면 worker node가 생성될 때마다 해당 shell도 함께 적용된다.이미 사용하고 있는 시작템플릿이 있는 경우 새로운 버전을 생성하여 적용한다. EKS 콘솔 접속 -&gt; 클러스터 -&gt; 노드그룹 -&gt; 시작템플릿 -&gt; 작업 -&gt; 템플릿 수정(새 버전 생성) -&gt; 이미지 및 기본 설정들은 기존 버전과 동일하게 -&gt; 제일 하단의 사용자데이터(User data)에 아래와 같이 입력 -&gt; 템플릿 생성MIME-Version: 1.0Content-Type: multipart/mixed; boundary=\"==MYBOUNDARY==\"--==MYBOUNDARY==#!/bin/bash -xeEKS_REGION='ap-northeast-2'EKS_CLUSTER_NAME='cluster-name'EKS_API_SERVER_ENDPOINT='https://cluster-api-server-endpoint.gr7.ap-northeast-2.eks.amazonaws.com'EKS_B64_CLUSTER_CA='Certificate authority'/etc/eks/bootstrap.sh \\ --apiserver-endpoint $EKS_API_SERVER_ENDPOINT \\ --b64-cluster-ca $EKS_B64_CLUSTER_CA \\ --kubelet-extra-args '--node-labels=node-type=self,ng=kbland-ekcl-stg-ap' \\ $EKS_CLUSTER_NAME# docker prunesudo echo \"0 7 * * 1 bash docker image prune\" &gt; /etc/cron.weekly/cleaner_image.sh &amp;&amp; chmod +x /etc/cron.weekly/cleaner_image.sh--==MYBOUNDARY==--템플릿을 새로 생성한 후 EKS 콘솔에서 시작템플릿 버전을 위에서 생성한 버전으로 변경한다.그 후 ssh -i ec2-key.pem worker-node-ip 으로 worker node에 접속하여 해당 파일이 잘 생성됐나 확인한다. cat /etc/cron.weekly/cleaner_image.sh위와 같은 작업을 통해 해당 현상을 해결하였다.참고 https://kubernetes.io/ko/docs/concepts/containers/images/#%EC%9D%B4%EB%AF%B8%EC%A7%80-%ED%92%80-pull-%EC%A0%95%EC%B1%85 https://docs.docker.com/storage/storagedriver/" }, { "title": "EKS Node Group Security Group에 NLB health check용 ingress rule 생성 방지하기", "url": "/posts/nlb-sg-rule/", "categories": "AWS, EKS", "tags": "aws, eks, nlb", "date": "2022-11-16 10:40:00 +0900", "snippet": "EKS Node Group Security Group에 NLB health check용 ingress rule 생성 방지하기상황EKS Node Group의 Security Group을 살펴보던 도중 AWS NLB로 생성한 k8s Service의 NodePort들이 여러 개 생성되어 있는 것을 발견했다.심지어는 service를 새로 생성했을 때 sg의 ingress rule이 모자른다는 에러 메세지가 뜨기도 했다. (rule 개수 limit에 도달해서) 그래서 k8s NodePort 범위인 30000 ~ 32767 포트 대역으로 열어주는 rule을 생성했음에도 불구하고 30000번대 포트의 ingress rule은 지속적으로 생성되었다.해결 방안AWS Load balancer controller를 사용하면 annotation을 추가하여 sg에 rule 추가되는 것을 방지할 수 있다. AWS에서는 ALB/NLB에 대한 오퍼레이션을 유연하게 지원하고자 AWS Load balancer controller라는 오픈소스 프로젝트를 통해 Kubernetes의 기본 service controller보다 AWS 리소스에 대해 더 다양한 기능을 제공하고 있다.AWS Load balancer controller 설치하기https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/aws-load-balancer-controller.htmlAWS Load balancer controller를 설치한 후 Service를 생성할 때 다음과 같이 ‘service.beta.kubernetes.io/aws-load-balancer-type: external’ annotation을 추가하면 AWS Load balancer controller를 이용하여 NLB를 생성할 수 있다.apiVersion: v1kind: Servicemetadata: name: nlb-sample-service namespace: nlb-sample-app annotations: service.beta.kubernetes.io/aws-load-balancer-type: external service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facingspec: ports: - port: 80 targetPort: 80 protocol: TCP type: LoadBalancer selector: app: nginx AWS Load balancer controller를 사용하지 않으면 기본적으로 Kubernetes의 In-tree Service controller를 사용하여 AWS NLB를 생성한 것이어서 해당 annotation은 사용할 수 없다.추가할 annotation service.beta.kubernetes.io/aws-load-balancer-manage-backend-security-group-rules: “false”apiVersion: v1kind: Servicemetadata: name: nlb-sample-service namespace: nlb-sample-app annotations: service.beta.kubernetes.io/aws-load-balancer-type: external service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing\tservice.beta.kubernetes.io/aws-load-balancer-manage-backend-security-group-rules: \"false\"spec: ports: - port: 80 targetPort: 80 protocol: TCP type: LoadBalancer selector: app: nginx위의 annotation을 추가 시 Controller가 Security group rule을 자동으로 추가/삭제하지 않기 때문에 꼭 Worker node의 Security group에 직접 Ingress rule을 넣어주어야 한다.정리Kubernetes의 In-tree service controller는 NLB와 관련된 Security group rule을 자동으로 추가/삭제한다. 자동으로 Rule이 추가/삭제되지 않도록 하기 위해서는 AWS Load balancer controller를 이용하여 NLB타입의 서비스를 생성해야 하고, 이 때 ‘service.beta.kubernetes.io/aws-load-balancer-manage-backend-security-group-rules: false’ annotation을 추가하여 자동 제어 기능을 diable 할 수 있다.참고: https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/service/nlb/ https://docs.aws.amazon.com/eks/latest/userguide/network-load-balancing.html https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/service/annotations/#manage-backend-sg-rules" }, { "title": "vim 에러 Cannot create backup file(add ! to overwrite) 해결하기", "url": "/posts/vim-backup-error/", "categories": "Linux", "tags": "linux, vim", "date": "2022-11-15 10:00:00 +0900", "snippet": "vim 에러 Cannot create backup file(add ! to overwrite) 해결하기상황vim 명령어로 파일을 수정하고 :wq로 저장하려 하면 아래와 같은 에러가 발생한다. Cannot create backup file(add ! to overwrite) 이런 에러는 !를 입력하면 해결할 수 있지만 파일을 수정할 때마다 매번 발생하므로 번거롭기 때문에 해당 에러 메세지가 나오지 않도록 해결해야 한다.해결 방법.vim 디렉토리에 backups 디렉토리를 생성해준다.cd ~/.vim mkdir backups이제 더이상 해당 에러 메세지가 출력되지 않고 :wq만으로도 수정이 잘 된다.참고 https://stackoverflow.com/questions/8428210/cannot-create-backup-fileadd-to-overwrite" }, { "title": "EKS에 Kubernetes Dashboard 배포하기", "url": "/posts/k8s-dashboard/", "categories": "Kubernetes, Cases", "tags": "k8s, eks", "date": "2022-11-15 10:00:00 +0900", "snippet": "EKS에 Kubernetes Dashboard 배포하기상황AWS EKS에 Kubernetes Dashboard를 배포하여 관리하고자 한다.현 사이트의 경우 보안상의 이유로 Proxy를 통해 Kubernetes Dashboard에 접속해야 한다.그래서 아래와 같은 경로를 통해 Kubernetes Dashboard Pod에 도달하게 된다. PC -&gt; Proxy Server(Nginx) -&gt; Ingress Controller -&gt; Ingress -&gt; Kubernetes Dashboard Service -&gt; Kubernetes Dashboard PodKubernetes Dashboard는 무조건 https로 접속해야 하는데 앞쪽의 Ingress 또는 Proxy server 등에서 ssl 처리를 해주면 Kubernetes Dashboard 자체는 http로 설정이 가능하다.그래서 나는 제일 앞단에 있는 Nginx에 ssl 설정을 해줄 것이다.Kubernetes Dashboard 배포하기먼저 Kubernetes Dashboard를 배포해보자.1.Kubernetes Dashboard yaml 파일을 다운받는다. wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.4.0/aio/deploy/recommended.yaml -O k8s-dashboard.yaml2.아래와 같이 포트 및 옵션을 수정하여 배포한다. 포트는 편의를 위해 모두 8001로 변경한다. 아래의 예시는 수정할 부분만 가져온 것이다.kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: ports: - port: 8001 # 포트 수정 targetPort: 8001 # 포트 수정 selector: k8s-app: kubernetes-dashboard---kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: securityContext: seccompProfile: type: RuntimeDefault containers: - name: kubernetes-dashboard image: kubernetesui/dashboard:v2.7.0 imagePullPolicy: Always ports: - containerPort: 8001 # 포트 수정 protocol: TCP args: #- --auto-generate-certificates # 주석 처리 - --namespace=kubernetes-dashboard # namespace를 설정한다\t\t\t- --insecure-bind-address=0.0.0.0 # 모든 http 접속을 허용한다 (추가) - --enable-insecure-login # http 로그인 허용한다 (추가) - --token-ttl=10800\t# 토큰 세션 유지시간 - 선택 사항임 # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTP # http로 변경 path: / port: 8001 # 포트 수정 initialDelaySeconds: 30 timeoutSeconds: 30위와 같이 변경한 후 배포한다. kubectl apply -f k8s-dashboard.yamlKubernetes Dashboard용 Ingress 배포하기위에서 배포한 Kubernetes Dashboard로 라우팅 되도록 Ingress를 배포한다.apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: kubernetes-dashboard namespace: kubernetes-dashboard annotations: kubernetes.io/ingress.class: \"nginx\" spec: rules: - host: k8s-dashboard.com http: \t paths: - backend: service: name: kubernetes-dashboard port: number: 9090\t\t path: / pathType: Prefix kubectl apply -f k8s-dashboard-ingress.yamlKubernetes Dashboard에 EKS 권한 부여하기기본적으로 Kubernetes Dashboard 사용자의 권한은 제한되어 있다.그래서 eks-admin이라는 이름으로 Service Account 및 Cluster Role Binding를 생성하고, 이를 사용하여 관리자 권한으로 Dashboard에 연결할 것이다.아래와 같이 Service Account와 Cluster Role Binding를 생성한다. apiVersion: v1kind: ServiceAccountmetadata: name: eks-admin namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: eks-adminroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: eks-admin namespace: kube-system kubectl apply -f eks-admin-service-account.yamlNginx에 ssl 설정하기Kubernetes Dashboard는 무조건 https 통신이 돼야 하는데 이를 nginx에서 처리할 수도 있다.그래서 openssl 명령어로 사설 인증서를 생성한 후 Nginx에 ssl 설정을 해보자. openssl 명령어로 key 발급하기# private key와 인증서를 발급한다. 아래 명령어를 입력하면 국가부터 회사까지 각종 정보를 입력해야 한다.$ openssl req -new -newkey rsa:2048 -nodes -keyout k8s-dashboard.key -out k8s-dashboard.csr# 위에서 발급한 private key와 csr 키를 통해 crt 키를 발급한다.$ openssl x509 -req -days 365 -in k8s-dashboard.csr -signkey k8s-dashboard.key -out k8s-dashboard.crt Nginx에 ssl 설정하기upstream k8s-dashboard { least_conn; # Ingress Controller Service IPs server 10.123.45.678:80; server 10.123.45.679:80;}server { # ssl on 대신 listen 구문 마지막에 ssl을 넣는 것으로 변경됨. listen 443 ssl; server_name k8s-dashboard.com; charset utf-8;\t\t access_log /etc/nginx/log/access.log; error_log /etc/nginx/log/error.log; \t\t#ssl 인증서 적용 ssl_certificate /etc/nginx/ssl/k8s-dashboard.crt; #생성된 인증서경로 ssl_certificate_key /etc/nginx/ssl/k8s-dashboard.key; #생성된 개인키 \t\tlocation / { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme;\t\t\t\t proxy_pass http://k8s-dashboard; }} Host 설정하기nginx에 ssl 설정까지 마쳤으면 이제 hosts 파일에 설정한 Kubernetes Dashboard 도메인과 Nginx 서버 IP를 매핑해줘야 한다.$ vi /etc/hosts# 웹서버 IP # Kubernetes Dashboard 도메인10.111.22.333 k8s-dashboard.comKubernetes Dashboard에 접속하기주의사항: 반드시 https로 접속해야 한다.위에서 설정한 도메인으로 접속하면 아래와 같은 UI가 뜬다. https://k8s-dashboard.com로그인은 위에서 생성한 eks-admin token 값으로 로그인을 할 것이다.# Token 확인 명령어kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}')# 출력 예시Name: eks-admin-token-b5zv4Namespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name=eks-admin kubernetes.io/service-account.uid=bcfe66ac-39be-11e8-97e8-026dce96b6e8Type: kubernetes.io/service-account-tokenData====ca.crt: 1025 bytesnamespace: 11 bytestoken: &lt;token 값&gt; 위에서 출력된 token 값을 복사하여 로그인 한다. 로그인 하면 위와 같은 화면이 뜬다. 여기서 원하는 리소스를 골라 확인할 수 있다.끝.참고 [https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/dashboard-tutorial.html] (https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/dashboard-tutorial.html) [https://csupreme19.github.io/devops/kubernetes/2021/03/04/kubernetes-dashboard.htm] (https://csupreme19.github.io/devops/kubernetes/2021/03/04/kubernetes-dashboard.html)" }, { "title": "helm 개념 및 기본 사용법", "url": "/posts/helm-basic/", "categories": "Kubernetes, Helm", "tags": "k8s, helm", "date": "2022-11-15 10:00:00 +0900", "snippet": "helm 개념 및 기본 사용법용어 Chart: 패키지 같은 것 Release: 차트를 배포한 인스턴스 -&gt; 여러 개 가능 built in object: 빌트 인 객체 -&gt; 객체를 가지고 yaml 파일의 값을 지정할 수 있다. ex) {{ .Release.Name }} 기본 명령어# Chart 생성하기helm create [chart명]# Release 이름 지정해서 생성하기helm install [release명] [chart_dir_path] # Release 이름은 자동으로 주고 생성하기helm install [chart_dir_path] --generate-name# Release 생성 dry-runhelm install [release명] [chart_dir_path] --dry-run --debug# Release 생성시 변수로 들어가는 값 직접 지정하기helm install [release명] [chart_dir_path] --set [key=value]# Release 목록 출력helm list# Release의 manifest 확인하기helm get manifest [release명] 참고 https://helm.sh/ko/docs/chart_template_guide/getting_started/ " }, { "title": "fluent-bit의 parser에 사용된 정규식 파악하기", "url": "/posts/regex/", "categories": "Kubernetes, Cases", "tags": "k8s, fluent-bit", "date": "2022-11-03 11:00:00 +0900", "snippet": "fluent-bit의 parser에 사용된 정규식 파악하기fluent-bit의 parser에 사용된 정규식 파악하기참고 https://idreamtbest.tistory.com/71 http://sweeper.egloos.com/3064808" }, { "title": "Fluent-bit의 parser에 사용된 정규식 파악하기", "url": "/posts/fluent-bit-parser/", "categories": "Kubernetes, Cases", "tags": "k8s", "date": "2022-11-03 10:00:00 +0900", "snippet": "Fluent-bit의 parser에 사용된 정규식 파악하기Fluent-bit의 parser란Parser는 …https://docs.fluentbit.io/manual/pipeline/filters/parserFluent-bit parser에 적용된 정규식 파헤쳐보기[PARSER] Name multiline Format regex Regex /(?&lt;time&gt;Dec \\d+ \\d+\\:\\d+\\:\\d+)(?&lt;message&gt;.*)/ Time_Key time Time_Format %b %d %H:%M:%S참고 https://idreamtbest.tistory.com/71 http://sweeper.egloos.com/3064808" }, { "title": "폐쇄망에서 busybox 설치하여 k8s Pod 배포하기", "url": "/posts/k8s-busybox-offline-install/", "categories": "Nginx", "tags": "k8s, nginx", "date": "2022-11-02 11:00:00 +0900", "snippet": "폐쇄망에서 busybox 설치하여 k8s Pod 배포하기 Nginx를 배포하기 위한 ConfigMap, Deployment, Service를 yaml 파일로 작성한다. 참고 : [https://hub.docker.com//nginx](https://hub.docker.com//nginx) " }, { "title": "Nginx 413 에러", "url": "/posts/Nginx-413-error/", "categories": "Nginx", "tags": "k8s, nginx", "date": "2022-11-01 11:00:00 +0900", "snippet": "Nginx 413 에러상황AWS S3로 파일을 이관하는 테스트 중 413 에러가 발생했다.파일이 S3로 업로드되는 라우팅경로는 다음과 같다. 인터넷 –&gt; Nginx-ingress-controller(AWS EKS에 배포) –&gt; k8s ingress –&gt; nginx-pod(proxy server) –&gt; upload-pod –&gt; AWS S3 bucket413 에러란?Nginx에 설정된 허용 파일 크기보다 요청한 파일의 크기가 더 커서 발생하는 에러이다.해결법이번 경우는 Nginx ingress controller에도 설정을 해야 하고, 프록시 서버로 사용 중인 Nginx Pod에도 설정을 해야 한다. Nginx에 설정 nginx.conf 파일에 client_max_body_size를 원하는 크기로 설정해주면 된다. client_max_body_size의 기본 값은 1M이다. http, server, location block 어디든 설정이 가능하다. http{ client_max_body_size 100M; ...} Nginx-ingress-controller에 설정 k8s에 배포한 ingress에 주석(annotation)으로 설정해주면 모든 ingress controller Pod에 적용된다. kubectl edit ingress ingress-name# kubectl edit 후 아래 annotation 추가nginx.ingress.kubernetes.io/proxy-body-size: 8m 참고 https://developer.mozilla.org/ko/docs/Web/HTTP/Status/413 https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#custom-max-body-size " }, { "title": "kubectl top 명령어로 Pod와 Node의 메트릭 확인하기", "url": "/posts/k8s-top/", "categories": "Kubernetes, Cases", "tags": "k8s", "date": "2022-10-31 10:00:00 +0900", "snippet": "kubectl top으로 Pod와 Node의 메트릭 확인하기Metrics-server 배포하기먼저, kubectl top 명령어를 사용하려면 metrics-server를 배포해야 한다.# 배포하기kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml# 배포 확인kubectl get deployment metrics-server -n kube-systemPod와 Node 메트릭 확인하기linux에서 top 명령어를 통해 프로세스의 cpu와 memory 사용률을 확인할 수 있는 것처럼 kubernetes상에서도 kubectl top 명령어를 통해 Pod와 Node의 cpu와 memory 사용률을 확인할 수 있다.# pod의 사용 중인 cpu, memory 확인하기kubectl top pod -n test-namespace# nginx pod의 메트릭만 출력하기kubectl top pod -n test-namespace | grep nginx # watch 명령어를 활용해 메트릭 모니터링 하기watch -n 1 'kubectl top pod -n test-namespace'# 특정 pod의 메트릭만 모니터링 하기watch -n 1 'kubectl top pod -n test-namespace | grep nginx' # node 메트릭 확인하기kubectl top node kubectl top pod 명령어로는 사용 중인 cpu, memory 수만 출력되지만 kubectl top node 명령어로는 사용률도 출력된다. 참고 https://github.com/kubernetes-sigs/metrics-server " }, { "title": "Stern을 설치하여 k8s pod 로그 쉽게 보기", "url": "/posts/stern/", "categories": "Kubernetes, Cases", "tags": "k8s", "date": "2022-10-20 10:00:00 +0900", "snippet": "Stern을 설치하여 k8s multiple pod 로그 쉽게 보기Stern이란?Stern은 Multiple Pods 또는 Multiple Containers 의 로그를 한 번에 볼 수 있게 하는 Golang 기반의 도구.기본적으로는 kubectl logs -n namespace pod_id 명령어로는 하나의 Pod 또는 하나의 Container의 로그만 가능해서 보통 여러 개의 Pod나 Container 로그를 보려면 터미널 창을 여러 개 띄워서 봤었다.그러나 이런 방식은 명령어도 여러 번 입력해야 하고 불편한 점이 많아 이번에 Stern을 통해 간단하게 로그를 보고자 한다.Stern 설치 과정 Golang 설치# Golang 설치sudo apt-get updatesudo apt-get install golang# 설치 확인which go# golang 환경변수 확인go env&gt;&gt; GOPATH가 설정되어 있지 않으면 원하는 dir 경로 설정# golang 환경변수 설정은 go env -w &lt;환경변수명&gt;=&lt;값&gt; 으로 한다. go env -w GOPATH=/home/user/go Golang에서는 GOPATH에 설정된 dir을 기준으로 의존성 라이브러리를 관리한다. 의존성 라이브러리를 설치하는 방법은 go get 명령어이고 해당 명령어를 실행하면 GAPATH 경로에 의존성 라이브러리들이 다운로드된다. Govendor 설치# govendor 다운로드go get -u github.com/kardianos/govendor Stern 설치# Stern 설치mkdir -p $GOPATH/src/github.com/werckercd $GOPATH/src/github.com/werckergit clone https://github.com/wercker/stern.git &amp;&amp; cd sterngovendor syncgo install#설치 확인sternStern 사용법# test라는 글자가 들어간 모든 pods의 log 보기stern test --all-namespacesstern test -n test-namespace# label이 nginx인 pods의 log 보기stern --all-namespaces -l run=nginxstern -n test-namespace -l run=nginx더 쉬운 Stern 설치법sudo curl -L -o /usr/local/bin/stern \\ https://github.com/wercker/stern/releases/download/1.10.0/stern_linux_amd64sudo chmod +x /usr/local/bin/stern 참고 https://github.com/wercker/stern https://devcenter.heroku.com/articles/go-dependencies-via-govendor https://blog.naver.com/alice_k106/221618543021 Stern 외 다른 Multi log tool 참고 https://stackoverflow.com/questions/62569038/how-to-get-logs-of-two-pods-without-opening-two-terminals-and-using-kubectl-tail " }, { "title": "AWS EKS Worker Node에 Customized된 설정 적용하기", "url": "/posts/EKS-template/", "categories": "AWS, EKS", "tags": "aws, eks", "date": "2022-10-20 10:00:00 +0900", "snippet": "AWS EKS Worker Node에 Customized된 설정 적용하기상황push 발송 등으로 인해 동접자 수 증가 대응을 위해 Nignx 기반의 서비스의 worker_conneections 값과 OS 커널 파라미터 수정이 필요하다. 즉, EKS Worker Node의 파라미터를 변경해야 한다. [변경할 파라미터 값] /etc/sysctl.conf fs.file-max = 3244429 etc/security/limits.conf ec2-user soft nofile 4096ec2-user hard nofile 10240 EKS Node Group 시작템플릿 변경하기 시작템플릿을 변경하는 이유? Worker Node 내에 Customized된 설정을 적용하기 위해서는 AWS EC2에서 제공하는 UserData라는 기능을 사용하여 인스턴스 시작 시 원하는 명령어가 실행될 수 있도록 해야 한다.그렇기 때문에 현재 Node Group의 시작템플릿에 UserData를 적용한 새로운 버전의 템플릿으로 업데이트하거나 템플릿 자체를 새로 생성하여 Worker Node를 재생성해야 한다.(1)시작 템플릿 버전 업데이트는 사용자 지정 시작 템플릿을 사용 하였을때만 가능(2)EKS Optimized AMI를 사용하는 경우 새로운 시작 템플릿을 생성하여 적용 나의 경우 (2)에 해당하므로 새로 UserData를 적용한 새로운 시작 템플릿을 생성하였다.[과정] 새로운 시작 템플릿을 생성한다. EC2 대쉬보드 -&gt; 인스턴스 -&gt; 시작 템플릿 -&gt; 시작 템플릿 생성 AMI는 현재 사용 중인 EKS Optimized AMI를 선택 및 나머지 세부 사항은 기존 설정 그대로 고급세부정보 -&gt; 사용자 데이터(UserData)에 bootstrap.sh 스크립트가 실행될 수 있도록 필요한 명령어와 함께 입력 -&gt; 생성 bootstrap.sh 스크립트가 실행될 수 있도록 입력하는 이유는 생성되는 인스턴스가 EKS 클러스터에 조인되도록 하기 위함이다. MIME-Version: 1.0Content-Type: multipart/mixed; boundary=\"==MYBOUNDARY==\"--==MYBOUNDARY==Content-Type: text/x-shellscript; charset=\"us-ascii\"#!/bin/bashecho 'fs.file-max = 3244429' &gt;&gt; /etc/sysctl.confecho 'ec2-user soft nofile 4096' &gt;&gt; /etc/security/limits.confecho 'ec2-user hard nofile 10240' &gt;&gt; /etc/security/limits.confset -ex/etc/eks/bootstrap.sh {클러스터 이름} --apiserver-endpoint {EKS API server endpoint} --b64-cluster-ca {Certificate authority}--==MYBOUNDARY==-- 방금 새로 생성한 시작 템플릿을 사용하는 새로운 노드 그룹을 생성한다. EKS 대쉬보드 -&gt; 클러스터 -&gt; 컴퓨팅 -&gt; 노드 그룹 -&gt; 노드 그룹 추가 시작 템플릿 사용 -&gt; 위에서 생성한 시작 템플릿 지정 및 세부 사항 입력 -&gt; 생성 새로운 노드 그룹의 노드가 모두 Ready 상태가 된 것을 확인한 후 필요하면 이전 노드 그룹을 삭제한다. 적용된 커널 파라미터 확인 cat /etc/sysctl.confcat /etc/security/limits.conf 주의할 점Node Group에 적용된 시작 템플릿과 해당 Node Groupd의 AutoScalingGroup(이하 ASG)이 사용하는 시작 템플릿이 별개로 있는데,변경해야 할 시작 템플릿은 Node Group 대쉬보드에서 바로 보이는 시작 템플릿이고 ASG에 할당된 시작 템플릿은 변경하지 않아야 한다.왜냐하면 ASG는 Managed Node Group 설정에 따라 EKS에서 자동으로 생성/변경하므로 사용자의 Custom한 변경이 이루어지지 않도록 AWS측에서 권장하기 때문이다. 참고 https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/eks-optimized-ami.html https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/launch-templates.html#launch-template-custom-ami https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/create-managed-node-group.html https://aws.amazon.com/ko/premiumsupport/knowledge-center/eks-resolve-node-group-errors-in-cluster/ " }, { "title": "AWS NLB로 배포한 K8s Service가 Pending 상태에서 지워지지 않는 문제", "url": "/posts/k8s-pending-error/", "categories": "Kubernetes, Errors", "tags": "k8s, aws", "date": "2022-10-08 11:00:00 +0900", "snippet": "AWS NLB로 배포한 K8s Service가 Pending 상태에서 지워지지 않는 문제 해결상황AWS Load Balancer Controller를 사용하기 위해 Service를 LB 타입(AWS NLB)으로 배포하였으나, External IP가 생성되지 않으면서 Pending 상태에서 멈춰있음시도kubectl delete svc -n kube-system aws-load-balancer-service --force 결과: 지워지지 않음kubectl edit svc -n kube-system aws-load-balancer-service&gt; Type을 NodePort, ClusterIP로 변경&gt; 지정된 온갖 Annotation을 지움&gt; 그 후 delete 시도 결과: 지워지지 않음해결법kubectl edit svc -n kube-system aws-load-balancer-service&gt; finalizer를 지움&gt; 그 후 delete 시도 결과: 드디어 지워짐!!! 참고: https://stackoverflow.com/questions/61931602/cannot-delete-kubernetes-service-with-no-deployment" }, { "title": "K8s coredns로 Nginx Proxy 설정하기", "url": "/posts/k8s-coredns/", "categories": "Kubernetes, Cases", "tags": "k8s", "date": "2022-10-08 10:00:00 +0900", "snippet": "K8s coredns로 Nginx Proxy 설정하기CoreDNS란?CoreDNS는 쿠버네티스 클러스터의 DNS 역할을 수행할 수 있는, 유연하고 확장 가능한 DNS 서버이다.즉, K8s 클러스터에서 Pod와 Service에 할당된 도메인을 통해 요청이 들어오는 경우 해당 도메인을 알맞은 리소스의 IP로 변환하여 통신이 가능하게 해주는 것. - Pod만 존재할 때: [파드의 IP주소].[네임스페이스명].pod.cluster.local - Pod를 Service로 노출시켰을 때: [파드의 IP주소].[서비스명].[네임스페이스명].svc.cluster-domain.example - [서비스명].[네임스페이스명].svc.cluster.local Nginx Pod의 Proxy 설정 예시apiVersion: v1kind: ConfigMapmetadata: name: gateway-configmap namespace: gateway-nsdata: TZ: 'UTC' gateway-conf: | # k8s coredns svc의 dns 입력 또는 coredns pod의 IP 입력 resolver kube-dns.kube-system.svc.cluster.local valid=5s; upstream test-svc{ server test-svc.test-ns.svc.cluster.local:80; keepalive 32; } server { listen 80; server_name k8s-test.com; #요청하는 Host URL proxy_http_version 1.1; # istio에서는 1.1이나 2만 사용하므로 1.1을 사용한다고 명시한다. proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Connection \"\"; client_max_body_size 300M; # /로 오는 요청을 위에서 upstream으로 설정한 test-svc로 보냄. location / { root /usr/share/nginx/html; index index.html index.html; proxy_pass http://test-svc; } } 위 config를 /etc/nginx/conf.d/ 하위에 마운트되도록 Deployment애 설정하면 된다.프록시 과정http://k8s-test.com/ 로 오는 요청이 http://test-svc 로 프록시패스 되고 해당 test-svc는 쿠버네티스 클러스터의 test-ns Namespace에 배포된 test-svc Service이므로 결과적으로는 test-svc에 연결된다. 참고 https://kubernetes.io/ko/docs/concepts/services-networking/dns-pod-service/ https://seungjuitmemo.tistory.com/271 https://arisu1000.tistory.com/27859 " }, { "title": "Nginx를 k8s에 배포하기", "url": "/posts/Nginx1/", "categories": "Nginx", "tags": "k8s, nginx", "date": "2022-06-22 11:00:00 +0900", "snippet": "Nginx 배포용 yaml 파일 Nginx를 배포하기 위한 ConfigMap, Deployment, Service를 yaml 파일로 작성한다.[Nginx ConfigMap]apiVersion: v1kind: ConfigMapmetadata:name: nginx-confdata:nginx.conf: | user nginx; worker_processes 3; error_log /var/log/nginx/error.log; events { worker_connections 10240; } http { log_format main 'remote_addr:$remote_addr\\t' 'time_local:$time_local\\t' 'method:$request_method\\t' 'uri:$request_uri\\t' 'host:$host\\t' 'status:$status\\t' 'bytes_sent:$body_bytes_sent\\t' 'referer:$http_referer\\t' 'useragent:$http_user_agent\\t' 'forwardedfor:$http_x_forwarded_for\\t' 'request_time:$request_time'; access_log\t/var/log/nginx/access.log main; server { listen 80; server_name _; location / { root html; index index.html index.htm; } } include /etc/nginx/virtualhost/virtualhost.conf; }virtualhost.conf: | upstream app { server localhost:8080; keepalive 1024; } server { listen 80 default_server; root /usr/local/app; access_log /var/log/nginx/app.access_log main; error_log /var/log/nginx/app.error_log; location / { proxy_pass http://app/; proxy_http_version 1.1; } }[Nginx Deployment]apiVersion: v1kind: ConfigMapmetadata:name: nginx-confdata:nginx.conf: | user nginx; worker_processes 3; error_log /var/log/nginx/error.log; events { worker_connections 10240; } http { log_format main 'remote_addr:$remote_addr\\t' 'time_local:$time_local\\t' 'method:$request_method\\t' 'uri:$request_uri\\t' 'host:$host\\t' 'status:$status\\t' 'bytes_sent:$body_bytes_sent\\t' 'referer:$http_referer\\t' 'useragent:$http_user_agent\\t' 'forwardedfor:$http_x_forwarded_for\\t' 'request_time:$request_time'; access_log\t/var/log/nginx/access.log main; server { listen 80; server_name _; location / { root html; index index.html index.htm; } } include /etc/nginx/virtualhost/virtualhost.conf; }virtualhost.conf: | upstream app { server localhost:8080; keepalive 1024; } server { listen 80 default_server; root /usr/local/app; access_log /var/log/nginx/app.access_log main; error_log /var/log/nginx/app.error_log; location / { proxy_pass http://app/; proxy_http_version 1.1; } }[Nginx Service]apiVersion: v1kind: Servicemetadata:name: nginx-svclabels: app: nginxspec:ports:- port: 80 # Service의 포트 targetport: 80 # Pod의 포트 protocol: TCPselector: app: nginx 참고 : [https://hub.docker.com//nginx](https://hub.docker.com//nginx)Nginx Forward Proxy 설정https://stackoverflow.com/questions/48921522/using-nginx-as-forward-proxy-in-kubernetes 참고: https://gist.github.com/petitviolet/d36f33d145d0bbf4b54eb187b79d0244" }, { "title": "ArgoCD를 K8s에 배포하기", "url": "/posts/ArgoCD1/", "categories": "CICD, ArgoCD", "tags": "k8s, ArgoCD", "date": "2022-06-22 11:00:00 +0900", "snippet": "ArgoCD Argo CD는 쿠버네티스 환경에서 지속적 전달을 통해 서비스를 배포하기 위한 전략을 도와주는 오픈소스 툴킷이다. Argo CD는 쿠버네티스 클러스터 내부에 Pod 형태로 배포된다. Argo CD는 특정 원격 저장소(GitOps Repository)의 내용을 감지하여 내용의 변경사항(Diff)이 발견되면 이를 사용자에게 알려주고 반영할지 여부를 물어본다. 특정 원격 저장소 : GitHub, Bitbucket, 또는 Gitlab과 같은 원격 Repository 안에 쿠버네티스에서 배포나 서비스 설정을 위해 사용하는 매니페스트 리소스(=yaml 파일)들을 관리하는 원격 저장소 Prequisite AWS EKS 클러스터 배포 Kubectl 설치ArgoCD 설치# argocd 네임스페이스 생성 kubectl create namespace argocd # Argo CD 배포kubectl apply -n argocd -f &lt;https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml&gt;# Argo CD API Server에서 외부 통신을 할 수 있도록 Argo CD의 Service의 Type을 Load Balancer로 변경(AWS에서는 Classic Load Balancer로 배포됨)kubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' Argo CD API Server가 대시보드를 제공한다. ⇒ 해당 Pod의 서비스를 통해 외부 접속을 해야 함 ⇒ Service 타입 LoadBalancer로 변경한다. (default는 ClusterIP이다) Argo CD CLI 설치curl -LO https://github.com/argoproj/argo-cd/releases/download/v1.4.2/argocd-linux-amd64chmod u+x argocd-linux-amd64sudo mv argocd-linux-amd64 /usr/local/bin/argocd## CLI 찾지 못하는 경우 Path 등록export PATH=/usr/local/bin:$PATHecho 'export PATH=/usr/local/bin:$PATH' &gt;&gt; ~/.bashrc## 설치 확인$ argocd Argo CD CLI는 Argo CD API Server에게 명령을 전달하여 Argo CD를 제어한다.Argo CD 초기 Admin 계정 설정ARGOCD_SERVER=`kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server -o name | cut -d'/' -f 2` ARGOCD_SERVER_HOST=`kubectl get svc argocd-server -o json -n argocd | jq -r '.status.loadBalancer.ingress[0].hostname'`# argocd 커맨드라인 인터페이스는 HTTP2/gRPC 프로토콜을 통해 통신하므로 # 로드밸런서 혹은 외부 통신을 담당하는 라우터가 HTTP2/gRPC 프로토콜을 완벽히 지원해야함.# 하지만 AWS에서 제공하는 ALB 혹은 Classic LB는 HTTP2/gRPC 프로토콜을 완벽하게 지원하지 않기 때문에# --grpc-web 이라는 옵션을 추가적으로 붙여줘야 함.$ argocd login &lt;ARGOCD_SERVER_HOST&gt;:80 --grpc-web # AWS EKS 혹은 HTTP2/gRPC 프로토콜을 완벽히 지원해 주지 않는 로드밸런서만 해당# 그 외의 경우$ argocd login &lt;ARGOCD_SERVER_HOST&gt;WARNING: server certificate had error: x509: certificate is valid for localhost, argocd-server, argocd-server.argocd, argocd-server.argocd.svc, argocd-server.argocd.svc.cluster.local, not a123456dd12ab11baba0a123a1234567-1234567890.ap-northeast-2.elb.amazonaws.com. Proceed insecurely (y/n)? yUsername: adminPassword: # ARGOCD_SERVER와 동일'admin' logged in successfullyContext 'a123456dd12ab11baba0a123a1234567-1234567890.ap-northeast-2.elb.amazonaws.com:80' updated# -------------------------------# default 계정 정보는 아래와 같음# username : admin# password : ARGOCD_SERVER와 동일# -------------------------------# Argo CD 비밀번호 변경$ argocd account update-password*** Enter current password: # ARGOCD_SERVER와 동일*** Enter new password: # 변경할 비밀번호 입력*** Confirm new password: # 변경할 비밀번호 재입력(확인)# Argo CD Endpoint$ kubectl get svc argocd-server -n argocd -o json | jq -r '.status.loadBalancer.ingress[0].hostname' 초기 계정 설정 후 브라우저를 통해 Load Balancer의 Endpoint로 접속하여 Argo CD 대시보드로 접속할 수 있다.⇒ 위에서 설정한 계정으로 로그인Repository 설정https://medium.com/finda-tech/eks-cluster%EC%97%90-argo-cd-%EB%B0%B0%ED%8F%AC-%EB%B0%8F-%EC%84%B8%ED%8C%85%ED%95%98%EB%8A%94-%EB%B2%95-eec3bef7b69bWebhook Webhook이란 특정 이벤트가 발생하였을 때 타 서비스나 응용프로그램으로 알림을 보내는 기능이다. 기본적으로 Argo CD에서는 3분 뒤에 Repository를 polling 하여 manifest의 업데이트를 감지한다 ⇒ Webhook 기능을 이용하면 Repository에 변경사항을 Push 하면 Webhook 이벤트가 트리거 되어 바로 Argo CD에서 Sync 되며 Webhook request log에서 확인 가능하다. " }, { "title": "Terraform으로 Infra 구성하기 No.4(Terraform - Attribute/Dependency/Output)", "url": "/posts/TF4/", "categories": "IaC, Terraform", "tags": "iac, terraform", "date": "2022-05-10 11:00:00 +0900", "snippet": "Resource Attribute Resource의 argument 값을 다른 resource의 argument 값으로 지정할 수 있다.[main.tf]resource \"local_file\" \"pet\" {filename = var.filenamecontent = \"My favorite pet is ${random_pet.my-pet.id}\"}resource \"random_pet\" \"my-pet\" {prefix = var.prefixseparator = var.separatorlength = var.length}=&gt; random_pet인 my-pet resource의 id를 local_file인 pet resource의 content 값으로 넣어주었다. config 파일에는 my-pet의 id가 없지만 terraform apply를 실행할 경우 random provider는 지정해준 argument들을 가지고 id를 만든다.Resource DependenciesImplicit Dependency[main.tf]resource \"local_file\" \"pet\" {filename = var.filenamecontent = \"My favorite pet is ${random_pet.my-pet.id}\"}resource \"random_pet\" \"my-pet\" {prefix = var.prefixseparator = var.separatorlength = var.length} ${random_pet.my-pet.id} 과 같은 표현을 사용하여 다른 resource의 argument 값을 가져다 쓸 수 있다. terraform apply 를 실행하게 되면 random_pet인 my-pet이 먼저 생성되고 그 다음으로 local_file인 pet이 생성된다. 삭제의 경우에는 그 반대. 이렇게 의존성을 갖게 하는 표현을 사용하여 의존성이 생기는 경우를 Implicit Dependency (암시적 의존성)라고 한다.Explicit Dependency[main.tf]resource \"local_file\" \"pet\" {filename = var.filenamecontent = \"My favorite pet is my dog\"depends_on = [ random_pet.my-pet]}resource \"random_pet\" \"my-pet\" {prefix = var.prefixseparator = var.separatorlength = var.length} 위와 같이 depends_on으로 의존성을 직접 명시하여 local_file.pet이 random_pet.my-pet 보다 나중에 생기게 할 수 있다. 이렇게 직접 명시하여 의존성을 정하는 경우 Explicit Dependency (명시적 의존성) 라고 한다.Output Variable Output Variable을 사용하면 Provisioning된 Resource의 세부사항을 빠르게 확인할 수 있다. Ansible이나 Shell script의 config에서 사용할 수 있다.[예시][main.tf]resource \"random_pet\" \"my-pet\" { length = var.length }output \"pet-name\" { value = random_pet.my-pet.id description = \"Record the value of pet ID generated by the random_pet resource\"}resource \"local_file\" \"welcome\" { filename = \"/root/message.txt\" content = \"Welcome to Kodekloud.\"}output \"welcome_message\" { value = local_file.welcome.content description = \"Welcome message from local_file welcome\"}[variable.tf]variable \"prefix\" { default = \"Mrs\"}variable \"separator\" { default = \".\"}variable \"length\" { default = \"1\"} output 블럭을 생성하여 출력할 variable을 지정한다. terraform init -&gt; terraform plan -&gt; terraform apply 로 적용하면 output 값도 출력된다. terraform output을 통해 output 블럭에서 지정한 값을 확인할 수 있다. 명령어 실행 결과 iac-server $ terraform output pet-name = ant welcome_message = Welcome to Kodekloud. " }, { "title": "Terraform으로 Infra 구성하기 No.3(Terraform - Variable)", "url": "/posts/TF3/", "categories": "IaC, Terraform", "tags": "iac, terraform", "date": "2022-05-10 10:00:00 +0900", "snippet": "Variable의 필요성 value를 하드코딩으로 작성하면 재사용성이 없어지기 때문에 IaC의 목적인 재사용성을 높이기 위해 변수를 사용한다.[hard codinf value - main.tf] resource \"local_file\" \"pet\" { filename = \"/root/pets.txt\" content = \"We love pets!\" } resource \"random_pet\" \"my-pet\" { prefix = \"Mrs\" separator = \".\" length = \"1\" }=&gt; Argument의 값들이 고정되어 있기 때문에 재사용이 어렵다.Variable 사용법[variables.tf]variable \"filename\" { default = \"/root/pets.txt\"}variable \"content\" { default = \"We love pets!\"}variable \"prefix\" { default = \"Mrs\"}variable \"separator\" { default = \".\"}variable \"length\" { default = \"1\"}[main.tf]resource \"local_file\" \"pet\" { filename = var.filename content = var.content}resource \"random_pet\" \"my-pet\" { prefix = var.prefix separator = var.separator length = var.length} variables.tf 파일을 따로 작성한 후 main.tf에서 해당 variable로 argument 값을 변경한다. terraform apply 실행시 하드코딩을 했을 경우와 동일하게 적용된다.Variable Update variables.tf 의 내용만 변경한 후 terraform apply 명령어를 실행하면 변경한 값으로 main.tf의 argument 값이 변경된다.Variable.tf 파일 구성기본 구성variable \"변수 이름\" { default = \"기본 값\" type = any(default)/string/number/bool(true/false) (선택요소임) description = \"변수에 대한 설명\" (선택요소임)}### Type의 종류 any : 기본 string : 문자열 default = “I’m learning terraform” number : 숫자 defualt = 1 bool : 참/거짓 defualt = true/false list : 인덱스를 가진 여러 개의 값 defualt = [“dog”, “cat”, “bird”] -&gt; index= [0,1,2] main.tf에서 변수.인덱스를 통해 argument 값을 지정 list(string) / list(number)와 같이 list의 type을 지정할 수 있다. list 예시[variables.tf]variable \"pets\" {default = [\"dog\", \"cat\", \"bird\"] type = list}[main.tf]resource \"local_file\" \"pet\" { pets = var.pets[0]} map : 여러 개의 key &amp; value 쌍 map도 list처럼 type을 지정할 수 있다. map(string) / map(number) map 예시[variables.tf]variable \"file-content\" { type = map default = { \"statement1\" = \"we love pets\" \"statement2\" = \"we love animals\" }}[main.tf]resource \"local_file\" \"pet\" { content = var.file-content[\"statement2\"]} set : 중복되지 않는 여러 개의 값. 인덱스가 있음. list와 동일한 구조지만 다른 점은 set은 중복된 값을 가질 수 없다는 점 set 틀린 예시[variables.tf]variable \"pets\" {default = [\"dog\", \"cat\", \"bird\", \"dog\"] type = list}=» 중복된 값이 있으므로 terraform apply 를 하면 에러가 발생한다.[main.tf]resource \"local_file\" \"pet\" { pets = var.pets[0]} object : 각기 다른 type을 가진 여러 개의 key &amp; value 쌍.object 예시[variables.tf]variable \"bella\" { type = object({ name = string color = string age = number food = list(string) favorite_pet = bool )} default = { name = \"bella\" color = \"brown\" age = 7 food = [\"fish\", \"chicken\", \"turkey\"] favorite_pet = true }}[main.tf]resource \"local_file\" \"pet\" { name = var.bella.name color = var.bella.color} tuples : list 형식으로 각기 다른 type을 가진 여러 개의 값을 나열tuples 예시[variables.tf]variable \"kitty\" {type = tuple([string, number, bool])default = [\"cat\", 7, true] }[main.tf]resource \"local_file\" \"pet\" { pets = var.pets[0]}Input Variable 방식 (변수 값 입력 방식) variables.tf 파일에 변수만 지정해 놓은 후 값은 따로 넣어주는 방식이다.command line flags (terraform 명령어로 입력) terrafom apply -var \"filename=/root/pets.txt\" -var \"content=We love pets!\" -var \"prefix=Mrs\" -var \"separator=.\" -var \"length=2\"Environment Variables (환경변수 지정) export TF_VAR_filename=\"/root/pets.txt\" export TF_VAR_content=\"We love pets!\" export TF_VAR_prefix=\"Mrs\" export TF_VAR_separator=\".\" export TF_VAR_length=\"2\" terraform applyVariable Definition Files (변수 지정 파일 따로 생성)[terraform.tfvars]filename= \"/root/pets.txt\"content = \"We love pets!\"prefix = \"Mrs\"separator = \".\"length = \"2\"=&gt; 이렇게 .tfvars/.tfvars.json 또는 *.auto.tfvars/*.auto.tfvars.json 파일을 만든 후 terraform apply 명령어를 실행하면 된다.Variable 적용 순서 및 우선순위 위에서 확인한 변수 적용 방법들을 동시에 사용했을 경우 적용되는 순서는 다음과 같다.[적용 순서] Environment Variables terraform.tfvars *.auto.tfvars (alphabetical order) -var or -var-file (command-line-flags) 위의 순서로 하나씩 적용되는데 순서와 우선순위는 반비례되기 때문에 먼저 적용된 것은 그 다음에 적용되는 것에 의해 덮어씌워진다.=&gt; 즉, 위의 4가지 방법으로 변수가 적용된다면 command-line-flags로 지정된 변수가 적용되는 것이다. [우선순위] -var or -var-file (command-line-flags) *.auto.tfvars (alphabetical order) terraform.tfvars Environment Variables" }, { "title": "Terraform으로 Infra 구성하기 No.2(Terraform - Provider)", "url": "/posts/TF2/", "categories": "IaC, Terraform", "tags": "iac, terraform", "date": "2022-05-09 16:00:00 +0900", "snippet": "Provider의 3가지 tiers Official: Hashicorp에 의해 관리되는 provider들 / 주요 클라우드 포함 (ex. local, AWS, GCP, Azure etc) Verified: Hashicorp와 파트너를 맺은 Third party 회사들에 의해 관리되는 provider Community : 개인들이 참여하는 Hashicorp 커뮤니티에 의해 관리되는 providerterraform init terraform init 명령어를 실행하면 terraform은 .tf 파일에 명시돼있는 provider의 플러그인을 모두 다운로드 한다. initialize가 모두 끝나면 어떤 provider의 어떤 버전을 다운로드 받았는지 확인할 수 있다. 다운로드된 플러그인은 workspace에 생성된 .terraform/plugin 디렉터리에 저장된다. 플러그인은 꾸준히 새로운 버전으로 업데이트 되며 init시에 새로운 버전으로 다운로드 된다. 만약 새로운 버전으로 다운로드 하고 싶지 않으면 config 파일에 명시하여 특정 버전을 다운로드 할 수 있다. 플러그인 이름 구성 : Hostname/Organizational Namespace/Type(provider name) ex) registry.terraform.io/hashicorp/local Multiple Providers 하나의 config에 여러 개의 provider를 설정할 수 있다.[예시 - main.tf] resource \"local_file\" \"data\" { filename = \"/root/k8s.txt\" content = \"kubernetes the hard way!\" } resource \"kubernetes_namespace\" \"dev\" { metadata { name = \"development\" } } 위 예시와 같이 local이라는 provider와 kubernetes라는 provider를 하나의 main.tf 파일에서 사용하고 있다. terraform init을 하게 되면 두 개의 provider 플러그인을 모두 다운로드 하게 된다. 만약 local 플러그인이 이미 다운로드 되어있는 경우, kubernetes 플러그인만 다운로드 한다." }, { "title": "Terraform으로 Infra 구성하기 No.1(Terraform 설치 및 명령어 사용법)", "url": "/posts/TF1/", "categories": "IaC, Terraform", "tags": "iac, terraform", "date": "2022-05-09 16:00:00 +0900", "snippet": "Terraform 설치하기 실습은 Ubuntu 환경에서 진행하였다. 다운로드 공식 문서 : https://www.terraform.io/downloads curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" sudo apt-get update &amp;&amp; sudo apt-get install terraform terraform version Terraform HCL 문법 Terraform은 HCL(HashiCorp configuration language)이라는 언어를 사용하며 .tf 확장자를 사용한다. 먼저 Block name을 지정한다. ex) resource 그 다음으로 사용하고자 하는 provider와 provider가 제공하는 resource를 언더바(_)로 연결하여 명시한다. 그리고 Resource의 이름을 넣어준다. 중괄호{ } 안에 Argument를 입력해주어야 하는데 Provider마다 각기 다른 Argument를 넣어주어야 하므로 공식문서를 살펴보며 입력한 후 저장해준다. Terraform은 local, private cloud뿐만 아니라 다양한 public cloud를 provider로 지원하고 있으며 provider에 따라 수많은 resource type과 argument가 존재한다.그렇기 때문에 provider마다 필요한 resource type과 argument들을 다 외우고 있을 필요가 없고, 잘 정리된 terraform 공식문서를 참조하여 config 파일을 작성하면 된다. 참고 문서: https://registry.terraform.io/browse/providers 하나의 terraform config 파일은 원하는 만큼 많은 수의 configuration block을 포함할 수 있다. =&gt; 즉, 하나의 파일로 여러 리소스를 생성할 수 있다.Terraform config file 이름에 따른 용도Terraform Workflow 4 Steps terraform은 Directory별로 명령어가 동작하여 현재 위치한 디렉터리에 있는 모든 .tf 확장자를 가진 파일을 모두 읽어들인 후, 리소스 생성, 수정, 삭제 작업을 진행한다.그러므로 실행하고자 하는 .tf 파일이 위치한 Workspace로 이동하여 terraform 명령어를 실행해야 한다. terraform config 파일을 작성한다. ex) local.tf init 명령어를 통해 terraform을 initializing 한다. $ terraform init plan 명령어를 통해 실행 계획을 미리 살펴본다. $ terraform plan apply 명령어를 통해 새로운 사항 또는 변경된 사항을 적용한다. $ terraform apply show 명령어를 통해 apply된 리소스의 세부사항을 확인한다. $ terraform show AWS 자원 생성 예시리소스 업데이트 및 삭제하기업데이트생성한 .tf 파일에 Argument를 추가한다 → terraform plan 명령어로 업데이트 계획을 살펴본다 → terraform apply 명령어를 입력하여 업데이트를 실행한다. apply 입력시 기존의 파일은 삭제되고 새로 적용된 파일이 생성된다. ex) 위에서 생성한 local.tf 파일의 Argument 블록 안에 file_permission = \"0700\" 를 추가한 후 terraform plan → terraform apply를 해준다.삭제terraform으로 생성한 인프라를 완전히 삭제하기 위해서는 terraform destroy 명령어를 사용한다.terraform destroy 명령어는 현재 디렉터리에서 생성된 모든 인프라를 삭제한다." }, { "title": "AWS EKS 구성 No.2(AWS IAM 생성,EKS 생성)", "url": "/posts/EKS2/", "categories": "AWS, EKS", "tags": "aws, eks", "date": "2022-05-06 16:00:00 +0900", "snippet": "본 글에서는 AWS에서 제공하는 Kubernetes managed Service인 EKS를 구성하는 방법에 대해 설명하고자 한다.AWS IAM 생성IAM이란?AWS Identity and Access Management(IAM)은 AWS 리소스에 대한 액세스를 안전하게 제어할 수 있는 웹 서비스이다. IAM을 사용하여 리소스를 사용하도록 인증(로그인) 및 권한 부여(권한 있음)된 대상을 제어한다.초기 AWS 계정 생성시에는 모든 AWS 서비스 및 리소스에 완전한 엑세스 권한이 있는 root 사용자로 시작한다.그러나 root 사용자로 다른 작업을 하는 것은 보안에 취약하기 때문에 EKS 및 관련 AWS 서비스를 관리할 수 있는 권한을 가진 계정을 만들어 root 계정 대신 해당 계정으로 EKS를 관리한다.참고 : https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/security-iam.htmlIAM 생성테스트용으로 administrator 정책을 사용하여 계정을 생성한다.생성 절차 AWS IAM 콘솔 → 좌측 메뉴 중 ‘사용자’ 클릭 → 우측 상단 ‘사용자 추가’ 클릭 → 사용자이름(User name*) 입력 ex) eks-test-user → AWS 액세스 유형 선택: 프로그래밍 방식(Programmatic access) → 권한 설정: 기존 정책(Attach existing policies directly) : administratorAccess 선택 → 태그 추가(Add tags)는 건너뛰어도 됨 → 사용자만들기(Create User) 버튼 클릭 → 사용자 생성 후 csv다운로드 -액세스ID/엑세스키 저장 (외부 유출 금지)IAM 계정을 Bastion Host에 등록하기$ aws configure AWS Access Key ID [None]: 위에서 만든 IAM 계정 id AWS Secret Access Key [None]: 위에서 만든 IAM 계정 key Default region name [None]: us-east-2 Default output format [None]: ENTER IAM 계정 등록 확인$ aws sts get-caller-identity { \"UserId\": \"AIDAQAOULBR~~\", \"Account\": \"12313213213\", \"Arn\": \"arn:aws:iam::1232132131:user/eks-test-user\" } IAM 계정까지 등록하였으면 이제 K8s manage server(=Bastion Host)에서 EKS를 생성할 준비가 완료된 것이다.EKS 생성하기 참고: https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/getting-started-eksctl.html https://eksctl.io/introduction/ 주의 : EKS요금은 시간당 0.01USD + t3.medium 시간 0.416 *2 USD 이다.eksctl 명령어로 EKS 클러스터 생성하기 eksctl 명령어로만 생성하기 eksctl create cluster \\ --name k8s-demo \\ --region &lt;region-name&gt; \\ --with-oidc \\ --ssh-access \\ --ssh-public-key &lt;keypair의 public키파일이름&gt; \\ --nodes 3 \\ --node-type t3.medium \\ --node-volume-size=20 \\ --managed config 파일을 이용하여 생성하기[config.yaml 예시] kind: ClusterConfigapiVersion: eksctl.io/v1alpha5metadata:  name: # cluster 이름  region: # cluster 생성 Region  version: 1.21 # 1.18~1.21(default)kubernetesNetworkConfig:  serviceIPv4CIDR: # Cluster에 할당할 IP 대역 지정 privateCluster:   enabled: true #fully-private cluster 생성을 위해 true로 설정  skipEndpointCreation: true #private EKS가 다른 AWS 서비스와 통신할 때 필요한 Endpoint를 이미 만들어 놓았으면 skip 할 수 있다.iam:  withOIDC: true vpc:   id: #vpc ID  cidr: #vpc IP 대역  subnets:    private:      [AZ이름]:        id: # subnet ID        cidr: # subnet IP 대역      [AZ이름]:        id: # subnet ID        cidr: # subnet IP 대역  clusterEndpoints:     privateAccess: true #API Server Endpoint를 private IP만 할당되게 함managedNodeGroups: # 공통 특성을 가진 node를 그룹화 함.  - name: ng-1 # nodeGroup 이름     amiFamily: AmazonLinux2 # default - AmazonLinux2     instanceType: m5.xlarge # node Instance Type     availabilityZones: [“eu-west-2a”, “AZ2”] # node 생성할 AZs     subnets: # node 생성할 subnets       - subnetID 1      - subnetID 2    desiredCapacity: 2    minsize: # Auto Scaling시 최소 노드 개수    maxsize: # Auto Scaling시 최대 노드 개수    volumeSize: # default - 80GiB (단위: GiB)    volumeType: gp3 # ex. gp2, gp3(default), io1, sc1, st1    privateNetworking: true # Private Subnets에만 node가 생성될 시 반드시 true로 설정해야 함.     labels:      key: value    ssh: # node instance에 ssh 접속하려면 설정해야 함      allow: true      publicKeyPath: # default - ‘~/.ssh/id_rsa.pub’    iam: # nodeGroup에 대한 정책      withAddonPolicies:        imageBuilder: true # ECR에 대한 권한 추가        autoScaler: true # cluster-autoscaling 권한 추가        externalDNS: true # Route53에 대한 external-DNS 권한 추가        efs: true # EFS 권한 추가        ebs: true # EBS 권한 추가         albIngeress: true # ALB Ingress 권한 추가        cloudWatch: true # Cloud Watch에 대한 권한 추가    securityGroups: # nodegroup에 지정할 SecurityGroup      attachIDs: [“sg-123”, “sg-321”]    maxPodsPerNode: # node당 최대 Pod 개수 제한availabilityZones: [“eu-west-2a”, “eu-west-2b”, “eu-west-2c”] # Cluster가 생성될 AZscloudWatch:  clusterLogging:    enableTypes: [*] # ex. “api”, “audit”, “authenticator”, “controllerManager”, “scheduler”, “all”    logRetentionInDays: # log 보유 기간 / ex. 1,3,5,7,14,30,60,90,120,150,180,365,400,545,731,1827,3653 생성할 AWS 환경에 맞게 위와 같이 config 파일을 생성한 후 아래의 명령어를 통해 EKS 클러스터를 생성한다.eksctl create cluster --config-file=[file_path]설치 완료 후 아래의 명령어를 통해 설치 확인을 한다. 하나의 Bastion Host에서 여러 클러스터를 생성한 경우 kubectl config 명령어를 통해 클러스터를 변경하여 관리할 수 있다.#생성한 클러스터의 worker nodes 확인하기 kubectl get nodes #현재 사용 중인 cluster 확인 kubectl config current-context #context 목록 확인 kubectl config get-contexts #cluster 변경하기 - 위에서 확인한 context 목록 중 사용하고자 하는 cluster를 포함한 context로 변경한다. kubectl config use-context [Context Name] #변경되었는지 확인 kubectl config current-contexts kubectl get nodes #kubeconfig 파일 수정하기 vi $HOME/.kube/config 인터넷에 액세스하지 않고 EKS를 생성하는 방법 참고: https://aws.amazon.com/ko/premiumsupport/knowledge-center/eks-cluster-node-group-private-network/ 여기까지 EKS를 생성하는 방법을 알아보았다.다음 포스트에서는 생성한 EKS에 필요한 AWS 서비스를 연동하는 방법을 알아볼 것이다." }, { "title": "AWS EKS 구성 No.1(EKS 개념, Bastion Host 구성)", "url": "/posts/EKS1/", "categories": "AWS, EKS", "tags": "aws, eks", "date": "2022-05-06 10:40:00 +0900", "snippet": "본 글에서는 AWS에서 제공하는 Kubernetes managed Service인 EKS를 구성하는 방법에 대해 설명하고자 한다.EKS란?Amazon Elastic Kubernetes Service(Amazon EKS)는 클라우드 또는 온프레미스에서 Kubernetes 애플리케이션을 실행하고 크기를 조정하는 관리형 컨테이너 서비스이다.참고: https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/what-is-eks.htmlEKS는 관리형이기 때문에 Control Plane을 사용자가 직접 관리할 필요가 없다. Master Node를 여러 가용 영역(AZ)에 걸쳐 생성하고 관리하므로 고가용성을 보장한다. ETCD를 백업할 필요가 없다. 알아서 백업하고 복구하는 방식이므로 Control Plane에서 장애가 발생했는지조차 사용자는 알 수 없을 것이라 생각한다.EKS와 함께 사용하는 AWS 서비스 예시AWS 서비스를 적용하여 손쉽게 쿠버네티스 환경을 구성할 수 있다. 아래에 EKS와 함께 사용되는 AWS 서비스 몇 가지를 예시로 들었다. AWS ECR(Elastic Container Registry) : 컨테이너 이미지를 저장하고 공유 및 배포할 수 있는 관리형 Docker 컨테이너 레지스트리이다. K8s에 Pod를 배포하기 위해 필요한 Docker 이미지를 빌드하여 ECR에 저장할 수 있으며, 저장된 이미지를 Pod로 배포할 수 있다. AWS ELB(Elastic Load Balancing) : Elastic Load Balancing은 둘 이상의 가용 영역에서 EC2 인스턴스, 컨테이너, IP 주소 등 여러 대상에 걸쳐 수신되는 트래픽을 자동으로 분산한다. ALB(Application Load Balancer) 또는 NLB(Network Load Balancer)를 생성하여 K8s에 배포된 Pod를 서비스할 수 있다. AWS EFS(Elastic File System) : EFS는 사용자의 사용량에 따라 자동으로 크기가 확장 및 축소되는 서버리스 파일 시스템이다. Storage Class를 EFS로 생성하고 PVC를 통해 Pod에 해당 파일 시스템을 Volume Mount 할 수 있다. Bastion Host를 생성하여 K8s 관리 서버로 구성하기Bastion Host란?Bastion Host란 침입 차단 소프트웨어가 설치되어 내부와 외부 네트워크 사이에서 일종의 게이트 역할을 수행하는 호스트를 뜻한다.Bastion Host는 접근 제어 기능과 더불어 게이트웨이로서 가상 서버(Proxy Server)의 설치, 인증, 로그 등을 담당하며 네트워크 보안상 가장 중요한 방화벽 호스트 역할을 한다.EKS를 관리하기 위해서는 이러한 Bastion Host에 kubectl, awscli, eksctl을 설치해서 사용한다.Bastion Host 생성 방법기본적인 EC2 생성하는 방법에 따라 리눅스 기반 인스턴스를 생성한다.인스턴스 생성 절차 :AWS EC2 Console 접속 → 좌측 메뉴 중 ‘인스턴스’ 클릭 → 우측 상단 인스턴스 시작 클릭 → 이름 입력 ex)eks_bastion → 애플리케이션 및 OS 이미지 : ubuntu 또는 red hat 등의 리눅스 기반 os 중 원하는 os 선택 → 인스턴스 유형: 원하는 인스턴스 유형 선택 ex) 프리티어로 사용한 가능한 t2.micro로도 문제 없음 → 키 페어 선택 또는 새로 생성 : 인스턴스에 접속하기 위한 key → 네트워크 설정: 보안 그룹에 ssh 접속을 위한 22번 port 열어두기 → 스토리지: 프리티어 범위까지만 지정 → 고급 세부 정보는 필요할 경우만 설정 → 인스턴스 시작 클릭하여 인스턴스를 생성한다.Bastion Host에 CLI 설치1-1) AWS CLI란명령줄 셸의 명령을 사용하여 AWS 서비스와 상호 작용할 수 있는 오픈 소스 도구이다. EKS 및 여러 서비스를 Bastion Host에서 생성하고 사용할 때 필요한 도구이다.1-2) aws cli 설치 $ sudo apt-get install -y unzip $ curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" $ unzip awscliv2.zip $ sudo ./aws/install #설치 확인 $ aws --version 참고: https://docs.aws.amazon.com/ko_kr/cli/latest/userguide/install-cliv2-linux.html2-1) eksctl이란eksctl은 AWS EKS에서 Kubernetes 클러스터를 가장 빠르고 쉽게 생성하고 관리할 수 있게 해주는 도구이다.2-2) eksctl 설치 $ curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp $ sudo mv /tmp/eksctl /usr/local/bin #설치 확인 $ eksctl version 참고: https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/eksctl.html3-1) kubectl이란Kubernetes에서 API 서버와 통신하기 위해 사용하는 도구이다. EKS도 K8s 서비스이기 때문에 kubectl 명령어를 사용하여 K8s 클러스터를 구성한다.3-2) kubectl 설치 $ curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.19.6/2021-01-05/bin/linux/amd64/kubectl $ chmod +x ./kubectl $ mkdir -p $HOME/bin &amp;&amp; cp ./kubectl $HOME/bin/kubectl &amp;&amp; export PATH=$PATH:$HOME/bin $ echo 'export PATH=$PATH:$HOME/bin' &gt;&gt; ~/.bashrc #설치 확인 $ kubectl version --short --client #자동완성 source &lt;(kubectl completion bash) echo \"source &lt;(kubectl completion bash)\" &gt;&gt; ~/.bashrc #alias 사용 alias k=kubectl complete -F __start_kubectl k 참고: https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/install-kubectl.htmlhttps://kubernetes.io/ko/docs/reference/kubectl/cheatsheet/Bastion Host에 aws cli, eksctl, kubectl을 모두 설치했다면 이제 EKS를 관리할 tool을 모두 준비한 것이다.다음 포스트에서 EKS를 생성하는 방법에 대해 알아보자." } ]
